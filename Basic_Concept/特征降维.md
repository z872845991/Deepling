# 特征降维

## Introduction

- sklearn中的降维演算法都被包括在模块`decomposition`中
  - `decomposition`模块为一个矩阵分解模块
- SVD(奇异值分解)和PCA(主成分分析)都属于矩阵分解算法中的入门算法，通过分解特征矩阵来进行降维。
- 降维(矩阵分解)的过程中追求既减少特征的数量，又保留大量有效信息的新特征矩阵
  - 将带有重复信息(特征与特征之间有线性相关)的特征合并
  - 删除无效信息(noise)的特征
- 如果一个特征的方差(Variance)很大，则可以说明这个特征带有大量信息
- 两种主要的降维方法；其矩阵分解的方法不同，信息量衡量指标不同，但都涉及大量矩阵运算
  - SVD
  - PCA
- 通常在能进行PCA降维的情况下，不会进行特征选择
  - 无法使用PCA降维的情况下才会做特征选择

## 主成分分析

- PCA使用的信息量衡量指标，就是样本方差(Variance)；又称可解释性方差，**其值越大，特征所带的信息量越多**
- 样本方差公式如下
  - $Var$代表一个特征的方差
  - $n$代表样本量
  - $x_i$代表一个特征中的每个样本的取值
  - $\hat{x}$代表该列样本的均值
- PCA本质就是将**已存在的特征进行压缩**，降维后的特征不是原本特征中的任何一个特征，而是**通过某些方式组合起来的新特征**
  - 新特征矩阵生成**不具有可读性**
  - 以PCA为代表的降维方法是属于特征创造(feature creation)
    - 在线性模型当中，不能使用PCA
      - 通常使用特征选择进行降维

## 奇异值分解

- 奇异值分解可以在不计算协方差矩阵等等复杂计算冗长的情况下，直接求出新特征空间和降维后的特征矩阵
- 奇异值分解在矩阵分解中会比PCA简单快速
- 奇异值分解的信息衡量指标为奇异值，比较复杂

